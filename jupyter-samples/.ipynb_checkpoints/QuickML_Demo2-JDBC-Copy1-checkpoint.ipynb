{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\zhongli\\.conda\\envs\\tf-gpu\n",
      "\n",
      "  added / updated specs:\n",
      "    - ipython-sql\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ipython-sql-0.3.9          |        py37_1000          26 KB  conda-forge\n",
      "    prettytable-0.7.2          |             py_3          15 KB  conda-forge\n",
      "    sqlalchemy-1.3.12          |   py37hfa6e2cd_0         1.8 MB  conda-forge\n",
      "    sqlparse-0.3.0             |             py_0          28 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  ipython-sql        conda-forge/win-64::ipython-sql-0.3.9-py37_1000\n",
      "  prettytable        conda-forge/noarch::prettytable-0.7.2-py_3\n",
      "  sqlalchemy         conda-forge/win-64::sqlalchemy-1.3.12-py37hfa6e2cd_0\n",
      "  sqlparse           conda-forge/noarch::sqlparse-0.3.0-py_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "sqlalchemy-1.3.12    | 1.8 MB    |            |   0% \n",
      "sqlalchemy-1.3.12    | 1.8 MB    |            |   1% \n",
      "sqlalchemy-1.3.12    | 1.8 MB    | ########## | 100% \n",
      "\n",
      "ipython-sql-0.3.9    | 26 KB     |            |   0% \n",
      "ipython-sql-0.3.9    | 26 KB     | ########## | 100% \n",
      "\n",
      "sqlparse-0.3.0       | 28 KB     |            |   0% \n",
      "sqlparse-0.3.0       | 28 KB     | ########## | 100% \n",
      "\n",
      "prettytable-0.7.2    | 15 KB     |            |   0% \n",
      "prettytable-0.7.2    | 15 KB     | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.7.10\n",
      "  latest version: 4.8.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes -c conda-forge ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\zhongli\\.conda\\envs\\tf-gpu\n",
      "\n",
      "  added / updated specs:\n",
      "    - sqlalchemy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2019.11.27 |                0         163 KB  anaconda\n",
      "    certifi-2019.11.28         |           py37_0         157 KB  anaconda\n",
      "    sqlalchemy-1.3.7           |   py37he774522_0         1.8 MB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.1 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  openssl            conda-forge::openssl-1.1.1d-hfa6e2cd_0 --> anaconda::openssl-1.1.1-he774522_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2019.11.~ --> anaconda::ca-certificates-2019.11.27-0\n",
      "  certifi                                       conda-forge --> anaconda\n",
      "  sqlalchemy         conda-forge::sqlalchemy-1.3.12-py37hf~ --> anaconda::sqlalchemy-1.3.7-py37he774522_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "ca-certificates-2019 | 163 KB    |            |   0% \n",
      "ca-certificates-2019 | 163 KB    | 9          |  10% \n",
      "ca-certificates-2019 | 163 KB    | ########## | 100% \n",
      "\n",
      "sqlalchemy-1.3.7     | 1.8 MB    |            |   0% \n",
      "sqlalchemy-1.3.7     | 1.8 MB    |            |   1% \n",
      "sqlalchemy-1.3.7     | 1.8 MB    | ########## | 100% \n",
      "\n",
      "certifi-2019.11.28   | 157 KB    |            |   0% \n",
      "certifi-2019.11.28   | 157 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.7.10\n",
      "  latest version: 4.8.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes -c anaconda sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. Setting and getting a global]\n",
      "The value of testglobal is  8888\n",
      "\n",
      "[2 Iterating over a global]\n",
      "walk forwards\n",
      "subscript= 1, value=8888\n",
      "subscript= 2, value=9999\n",
      "\n",
      "[3. Calling a class method]\n",
      "Jan 15, 2020\n"
     ]
    }
   ],
   "source": [
    "import irisnative\n",
    "\n",
    "# Modify connection info based on your environment\n",
    "ip = \"192.168.99.101\"\n",
    "port = 9091\n",
    "namespace = \"USER\"\n",
    "username = \"SuperUser\" #\"_SYSTEM\"\n",
    "password = \"SYS\"\n",
    "\n",
    "# create database connection and IRIS instance\n",
    "connection = irisnative.createConnection(ip,port,namespace,username,password)\n",
    "dbnative = irisnative.createIris(connection)\n",
    "\n",
    "print(\"[1. Setting and getting a global]\")\n",
    "# setting and getting a global\n",
    "# ObjectScript equivalent: set ^testglobal(\"1\") = 8888\n",
    "dbnative.set(8888, \"testglobal\", \"1\")\n",
    "\n",
    "# ObjectScript equivalent: set globalValue = $get(^testglobal(\"1\"))\n",
    "globalValue = dbnative.get(\"testglobal\",\"1\")\n",
    "\n",
    "print(\"The value of testglobal is \", globalValue)\n",
    "print()\n",
    "\n",
    "print(\"[2 Iterating over a global]\")\n",
    "# modify global to iterate over\n",
    "# ObjectScript equivalent: set ^testglobal(\"1\") = 8888\n",
    "# ObjectScript equivalent: set ^testglobal(\"2\") = 9999\n",
    "dbnative.set(8888, \"testglobal\", \"1\")\n",
    "dbnative.set(9999, \"testglobal\", \"2\")\n",
    "\n",
    "Iter = dbnative.iterator(\"testglobal\")\n",
    "print(\"walk forwards\")\n",
    "for subscript, value in Iter.items():\n",
    "    print(\"subscript= {}, value={}\".format(subscript, value))\n",
    "print()\n",
    "\n",
    "print(\"[3. Calling a class method]\")\n",
    "# calling a class  method\n",
    "# ObjectScript equivalent: set returnValue = ##class(%Library.Utility).Date(5)\n",
    "returnValue = dbnative.classMethodValue(\"%Library.Utility\", \"Date\", 5)\n",
    "print(returnValue)\n",
    "\n",
    "# close connection\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0002\u0001\n",
      "\bffffff\u0014@\n",
      "\b\u0000\u0000\u0000\u0000\u0000\u0000\f",
      "@\n",
      "\bffffffö?\n",
      "\bÉ?\r",
      "\u0001Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "import irisnative\n",
    "\n",
    "# Modify connection info based on your environment\n",
    "ip = \"192.168.99.101\"\n",
    "\n",
    "port = 9091\n",
    "namespace = \"USER\"\n",
    "username = \"SuperUser\" #\"_SYSTEM\"\n",
    "password = \"SYS\"\n",
    "\n",
    "# create database connection and IRIS instance\n",
    "connection = irisnative.createConnection(ip,port,namespace,username,password)\n",
    "dbnative = irisnative.createIris(connection)\n",
    "\n",
    "#print(\"[4. Calling a SQL]\")\n",
    "# calling a SQL  method\n",
    "# ObjectScript equivalent: \n",
    "# set result=$SYSTEM.SQL.Execute(\"select top 5 name,dob,ssn from sample.person order by name\")\n",
    "# query = \"Select * FROM DataMining.IrisDataset\"\n",
    "# returnValue = dbnative.classMethodValue(\"$SYSTEM.SQL\", \"Execute\", query)\n",
    "# print(returnValue)\n",
    "\n",
    "globalValue = dbnative.get(\"DataMining.IrisDatasetD\", \"1\")\n",
    "print(globalValue)\n",
    "\n",
    "# close connection\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@thucnc/pyspark-in-jupyter-notebook-working-with-dataframe-jdbc-data-sources-6f3d39300bf6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars file:///C:/InterSystems/IRIS20194/dev/java/lib/JDK18 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"jdbc:IRIS://192.168.99.101:9091/USER\"\n",
    "properties = {\n",
    "    \"driver\": \"com.intersystems.jdbc.IRISDriver\",\n",
    "    \"user\": \"SUPERUSER\",\n",
    "    \"password\": \"SYS\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Progra~2\\Java\\jre1.8.0_211\n",
      "C:\\InterSystems\\IRIS20194\\dev\\java\\lib\\JDK18\n",
      "C:\\Progra~2\\Java\\jre1.8.0_211\n"
     ]
    }
   ],
   "source": [
    "os.environ['JAVA_HOME']='C:\\Progra~2\\Java\\jre1.8.0_211'\n",
    "os.environ['CLASSPATH']='C:\\InterSystems\\IRIS20194\\dev\\java\\lib\\JDK18'\n",
    "os.environ['HADOOP_HOME']='C:\\Progra~2\\Java\\jre1.8.0_211'\n",
    "\n",
    "\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['CLASSPATH'])\n",
    "print(os.environ['HADOOP_HOME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  Row(age=22, height=165, name='Vinay')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "a = Row(name = 'Vinay' , age=22 , height=165)\n",
    "print(\"a: \",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_211\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_211-b12)\n",
      "Java HotSpot(TM) Client VM (build 25.211-b12, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!java -version -help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect into Iris by PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+-----------+----------+-----------+\n",
      "| ID|PetalLength|PetalWidth|SepalLength|SepalWidth|    Species|\n",
      "+---+-----------+----------+-----------+----------+-----------+\n",
      "|  1|        1.4|       0.2|        5.1|       3.5|Iris-setosa|\n",
      "|  2|        1.4|       0.2|        4.9|       3.0|Iris-setosa|\n",
      "|  3|        1.3|       0.2|        4.7|       3.2|Iris-setosa|\n",
      "|  4|        1.5|       0.2|        4.6|       3.1|Iris-setosa|\n",
      "|  5|        1.4|       0.2|        5.0|       3.6|Iris-setosa|\n",
      "|  6|        1.7|       0.4|        5.4|       3.9|Iris-setosa|\n",
      "|  7|        1.4|       0.3|        4.6|       3.4|Iris-setosa|\n",
      "|  8|        1.5|       0.2|        5.0|       3.4|Iris-setosa|\n",
      "|  9|        1.4|       0.2|        4.4|       2.9|Iris-setosa|\n",
      "| 10|        1.5|       0.1|        4.9|       3.1|Iris-setosa|\n",
      "| 11|        1.5|       0.2|        5.4|       3.7|Iris-setosa|\n",
      "| 12|        1.6|       0.2|        4.8|       3.4|Iris-setosa|\n",
      "| 13|        1.4|       0.1|        4.8|       3.0|Iris-setosa|\n",
      "| 14|        1.1|       0.1|        4.3|       3.0|Iris-setosa|\n",
      "| 15|        1.2|       0.2|        5.8|       4.0|Iris-setosa|\n",
      "| 16|        1.5|       0.4|        5.7|       4.4|Iris-setosa|\n",
      "| 17|        1.3|       0.4|        5.4|       3.9|Iris-setosa|\n",
      "| 18|        1.4|       0.3|        5.1|       3.5|Iris-setosa|\n",
      "| 19|        1.7|       0.3|        5.7|       3.8|Iris-setosa|\n",
      "| 20|        1.5|       0.3|        5.1|       3.8|Iris-setosa|\n",
      "+---+-----------+----------+-----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME']='C:\\Progra~2\\Java\\jre1.8.0_211'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars file:///C:/InterSystems/IRIS20194/dev/java/lib/JDK18/intersystems-jdbc-3.0.0.jar pyspark-shell'\n",
    "os.environ['HADOOP_HOME']='C:\\Progra~2\\Java\\jre1.8.0_211'\n",
    "\n",
    "url = \"jdbc:IRIS://192.168.99.101:9091/USER\"\n",
    "properties = {\n",
    "    \"driver\": \"com.intersystems.jdbc.IRISDriver\",\n",
    "    \"user\": \"SUPERUSER\",\n",
    "    \"password\": \"SYS\"\n",
    "}\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#sc = SparkContext(appName='IRIS')\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.jdbc(url=url,\\\n",
    "    table=\"DataMining.IrisDataset\", \\\n",
    "    properties=properties) \n",
    "df.show()\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+-----------+----------+-----------+\n",
      "| ID|PetalLength|PetalWidth|SepalLength|SepalWidth|    Species|\n",
      "+---+-----------+----------+-----------+----------+-----------+\n",
      "|  1|        1.4|       0.2|        5.1|       3.5|Iris-setosa|\n",
      "|  2|        1.4|       0.2|        4.9|       3.0|Iris-setosa|\n",
      "|  3|        1.3|       0.2|        4.7|       3.2|Iris-setosa|\n",
      "|  4|        1.5|       0.2|        4.6|       3.1|Iris-setosa|\n",
      "|  5|        1.4|       0.2|        5.0|       3.6|Iris-setosa|\n",
      "|  6|        1.7|       0.4|        5.4|       3.9|Iris-setosa|\n",
      "|  7|        1.4|       0.3|        4.6|       3.4|Iris-setosa|\n",
      "|  8|        1.5|       0.2|        5.0|       3.4|Iris-setosa|\n",
      "|  9|        1.4|       0.2|        4.4|       2.9|Iris-setosa|\n",
      "| 10|        1.5|       0.1|        4.9|       3.1|Iris-setosa|\n",
      "| 11|        1.5|       0.2|        5.4|       3.7|Iris-setosa|\n",
      "| 12|        1.6|       0.2|        4.8|       3.4|Iris-setosa|\n",
      "| 13|        1.4|       0.1|        4.8|       3.0|Iris-setosa|\n",
      "| 14|        1.1|       0.1|        4.3|       3.0|Iris-setosa|\n",
      "| 15|        1.2|       0.2|        5.8|       4.0|Iris-setosa|\n",
      "| 16|        1.5|       0.4|        5.7|       4.4|Iris-setosa|\n",
      "| 17|        1.3|       0.4|        5.4|       3.9|Iris-setosa|\n",
      "| 18|        1.4|       0.3|        5.1|       3.5|Iris-setosa|\n",
      "| 19|        1.7|       0.3|        5.7|       3.8|Iris-setosa|\n",
      "| 20|        1.5|       0.3|        5.1|       3.8|Iris-setosa|\n",
      "+---+-----------+----------+-----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o135.jdbc.\n: java.sql.SQLException: [SQLCODE: <-30>:<Table or view not found>]\r\n[Location: <Prepare>]\r\n[%msg: < Table 'SQLUSER.BAZ' not found>]\r\n\tat com.intersystems.jdbc.IRISConnection.getServerError(IRISConnection.java:1344)\r\n\tat com.intersystems.jdbc.IRISConnection.processError(IRISConnection.java:1502)\r\n\tat com.intersystems.jdbc.InStream.readMessage(InStream.java:262)\r\n\tat com.intersystems.jdbc.InStream.readMessage(InStream.java:164)\r\n\tat com.intersystems.jdbc.InStream.readMessage(InStream.java:146)\r\n\tat com.intersystems.jdbc.IRISPreparedStatement.prepareInternal(IRISPreparedStatement.java:592)\r\n\tat com.intersystems.jdbc.IRISPreparedStatement.prepare(IRISPreparedStatement.java:551)\r\n\tat com.intersystems.jdbc.IRISPreparedStatement.<init>(IRISPreparedStatement.java:22)\r\n\tat com.intersystems.jdbc.IRISConnection.getOrCreatePossiblyShardedIRISPreparedStatement(IRISConnection.java:2307)\r\n\tat com.intersystems.jdbc.IRISConnection.prepareStatement(IRISConnection.java:1804)\r\n\tat com.intersystems.jdbc.IRISConnection.prepareStatement(IRISConnection.java:401)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:210)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)\r\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:238)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-65086f4ebff3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m df = sqlContext.read.jdbc(url=url,\\\n\u001b[0;32m      2\u001b[0m     \u001b[0mtable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"baz\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     properties=properties) \n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[1;34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[0mjpredicates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mString\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjpredicates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o135.jdbc.\n: java.sql.SQLException: [SQLCODE: <-30>:<Table or view not found>]\r\n[Location: <Prepare>]\r\n[%msg: < Table 'SQLUSER.BAZ' not found>]\r\n\tat com.intersystems.jdbc.IRISConnection.getServerError(IRISConnection.java:1344)\r\n\tat com.intersystems.jdbc.IRISConnection.processError(IRISConnection.java:1502)\r\n\tat com.intersystems.jdbc.InStream.readMessage(InStream.java:262)\r\n\tat com.intersystems.jdbc.InStream.readMessage(InStream.java:164)\r\n\tat com.intersystems.jdbc.InStream.readMessage(InStream.java:146)\r\n\tat com.intersystems.jdbc.IRISPreparedStatement.prepareInternal(IRISPreparedStatement.java:592)\r\n\tat com.intersystems.jdbc.IRISPreparedStatement.prepare(IRISPreparedStatement.java:551)\r\n\tat com.intersystems.jdbc.IRISPreparedStatement.<init>(IRISPreparedStatement.java:22)\r\n\tat com.intersystems.jdbc.IRISConnection.getOrCreatePossiblyShardedIRISPreparedStatement(IRISConnection.java:2307)\r\n\tat com.intersystems.jdbc.IRISConnection.prepareStatement(IRISConnection.java:1804)\r\n\tat com.intersystems.jdbc.IRISConnection.prepareStatement(IRISConnection.java:401)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:210)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)\r\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:238)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.jdbc(url=url,\\\n",
    "    table=\"baz\", \\\n",
    "    properties=properties) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\zhongli\\.conda\\envs\\tf-gpu\n",
      "\n",
      "  added / updated specs:\n",
      "    - pyspark\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    py4j-0.10.7                |             py_1         177 KB  conda-forge\n",
      "    pyspark-2.4.4              |             py_0       204.9 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       205.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  py4j               conda-forge/noarch::py4j-0.10.7-py_1\n",
      "  pyspark            conda-forge/noarch::pyspark-2.4.4-py_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "pyspark-2.4.4        | 204.9 MB  |            |   0% \n",
      "pyspark-2.4.4        | 204.9 MB  |            |   0% \n",
      "pyspark-2.4.4        | 204.9 MB  | 1          |   2% \n",
      "pyspark-2.4.4        | 204.9 MB  | 3          |   4% \n",
      "pyspark-2.4.4        | 204.9 MB  | 5          |   5% \n",
      "pyspark-2.4.4        | 204.9 MB  | 6          |   7% \n",
      "pyspark-2.4.4        | 204.9 MB  | 8          |   8% \n",
      "pyspark-2.4.4        | 204.9 MB  | 9          |  10% \n",
      "pyspark-2.4.4        | 204.9 MB  | #1         |  11% \n",
      "pyspark-2.4.4        | 204.9 MB  | #2         |  13% \n",
      "pyspark-2.4.4        | 204.9 MB  | #5         |  15% \n",
      "pyspark-2.4.4        | 204.9 MB  | #7         |  18% \n",
      "pyspark-2.4.4        | 204.9 MB  | #9         |  20% \n",
      "pyspark-2.4.4        | 204.9 MB  | ##2        |  22% \n",
      "pyspark-2.4.4        | 204.9 MB  | ##4        |  25% \n",
      "pyspark-2.4.4        | 204.9 MB  | ##7        |  27% \n",
      "pyspark-2.4.4        | 204.9 MB  | ##9        |  30% \n",
      "pyspark-2.4.4        | 204.9 MB  | ###2       |  32% \n",
      "pyspark-2.4.4        | 204.9 MB  | ###4       |  35% \n",
      "pyspark-2.4.4        | 204.9 MB  | ###7       |  37% \n",
      "pyspark-2.4.4        | 204.9 MB  | ###9       |  40% \n",
      "pyspark-2.4.4        | 204.9 MB  | ####2      |  42% \n",
      "pyspark-2.4.4        | 204.9 MB  | ####4      |  45% \n",
      "pyspark-2.4.4        | 204.9 MB  | ####7      |  47% \n",
      "pyspark-2.4.4        | 204.9 MB  | #####      |  50% \n",
      "pyspark-2.4.4        | 204.9 MB  | #####2     |  53% \n",
      "pyspark-2.4.4        | 204.9 MB  | #####5     |  55% \n",
      "pyspark-2.4.4        | 204.9 MB  | #####7     |  58% \n",
      "pyspark-2.4.4        | 204.9 MB  | #####9     |  60% \n",
      "pyspark-2.4.4        | 204.9 MB  | ######2    |  62% \n",
      "pyspark-2.4.4        | 204.9 MB  | ######5    |  65% \n",
      "pyspark-2.4.4        | 204.9 MB  | ######7    |  68% \n",
      "pyspark-2.4.4        | 204.9 MB  | #######    |  70% \n",
      "pyspark-2.4.4        | 204.9 MB  | #######2   |  73% \n",
      "pyspark-2.4.4        | 204.9 MB  | #######5   |  75% \n",
      "pyspark-2.4.4        | 204.9 MB  | #######7   |  78% \n",
      "pyspark-2.4.4        | 204.9 MB  | ########   |  81% \n",
      "pyspark-2.4.4        | 204.9 MB  | ########3  |  83% \n",
      "pyspark-2.4.4        | 204.9 MB  | ########5  |  86% \n",
      "pyspark-2.4.4        | 204.9 MB  | ########8  |  88% \n",
      "pyspark-2.4.4        | 204.9 MB  | #########  |  91% \n",
      "pyspark-2.4.4        | 204.9 MB  | #########3 |  93% \n",
      "pyspark-2.4.4        | 204.9 MB  | #########6 |  96% \n",
      "pyspark-2.4.4        | 204.9 MB  | #########9 |  99% \n",
      "pyspark-2.4.4        | 204.9 MB  | ########## | 100% \n",
      "\n",
      "py4j-0.10.7          | 177 KB    |            |   0% \n",
      "py4j-0.10.7          | 177 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.7.10\n",
      "  latest version: 4.8.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes -c conda-forge pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-23657fe0ba52>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-23657fe0ba52>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    export PYSPARK_SUBMIT_ARGS=\"--master local[2] pyspark-shell\"\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "export PYSPARK_SUBMIT_ARGS=\"--master local[2] pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d15fd62996b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"IRIS\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"IRIS JDBC QuickML Demo2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.sql.shuffle.partitions\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"com.intersystems.jdbc.IRISDriver\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mJVM\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \"\"\"\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_launch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36m_launch_gateway\u001b[1;34m(conf, insecure)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"IRIS\") \\\n",
    "    .appName(\"IRIS JDBC QuickML Demo2\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "driver = \"com.intersystems.jdbc.IRISDriver\"\n",
    "#path = \"data/flight-data/jdbc/my-sqlite.db\"\n",
    "#url = \"jdbc:sqlite:\" + path\n",
    "url = \"jdbc:IRIS://192.168.99.101:9091/USER\"\n",
    "tablename = \"DataMining.IrisDataset\"\n",
    "properties = {\n",
    "    \"driver\": \"com.intersystems.jdbc.IRISDriver\",\n",
    "    \"user\": \"SUPERUSER\",\n",
    "    \"password\": \"SYS\"\n",
    "}\n",
    "dbDataFrame = spark.read.format(\"jdbc\")\\\n",
    "            .option(\"url\", url)\\\n",
    "            .option(\"dbtable\", tablename)\\\n",
    "            .option(\"driver\", driver)\\\n",
    "            .load()\n",
    "dbDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
